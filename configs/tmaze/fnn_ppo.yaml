parameters:
    main:
        env: 'tmaze'
        name: 'environments:tmaze-v0'
        algorithm: 'PPO'
        num_workers: 1
        n_stack: 30
        eval_episodes: 100
        eval_deterministic: false
        eval_freq: 1.0e+3
        total_steps: 1.0e+5
        epsilon: 0.1
        beta: 1.0e-2
        learning_rate: 1.0e-3
        hidden_size: 128
        hidden_size_2: 128
        batch_size: 64
        rollout_steps: 128
